{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 879,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGGpu3ijLlPR",
        "outputId": "459a01e3-c2af-442f-918d-78f70559755f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               X1         X2          X3          X4          X5          X6  \\\n",
            "count  613.000000  613.00000  613.000000  613.000000  613.000000  613.000000   \n",
            "mean     0.740930  690.71615  309.747145  190.484502    4.807504    3.499184   \n",
            "std      0.092623   81.93815   34.856517   39.476913    1.694515    1.118216   \n",
            "min      0.620000  563.50000  245.000000  122.500000    3.500000    2.000000   \n",
            "25%      0.660000  612.50000  294.000000  147.000000    3.500000    3.000000   \n",
            "50%      0.710000  710.50000  318.500000  220.500000    3.500000    3.000000   \n",
            "75%      0.820000  759.50000  343.000000  220.500000    7.000000    4.000000   \n",
            "max      0.900000  808.50000  367.500000  220.500000    7.000000    5.000000   \n",
            "\n",
            "               X7          X8          Y1          Y2  \n",
            "count  613.000000  613.000000  613.000000  613.000000  \n",
            "mean     0.229527    2.810767   19.542049   22.026623  \n",
            "std      0.133214    1.558263    8.900325    8.437464  \n",
            "min      0.000000    0.000000    6.010000   10.900000  \n",
            "25%      0.100000    1.000000   12.630000   15.100000  \n",
            "50%      0.250000    3.000000   15.230000   17.640000  \n",
            "75%      0.400000    4.000000   28.050000   29.560000  \n",
            "max      0.400000    5.000000   42.110000   43.330000  \n",
            "               X1          X2          X3          X4          X5          X6  \\\n",
            "count  613.000000  613.000000  613.000000  613.000000  613.000000  613.000000   \n",
            "mean    -0.000062   -0.000444   -0.000153   -0.000323   -0.000199    0.000021   \n",
            "std      1.000150    0.999980    1.000169    0.999962    1.000248    1.000021   \n",
            "min     -1.306000   -1.553000   -1.858000   -1.722000   -0.772000   -1.341000   \n",
            "25%     -0.874000   -0.955000   -0.452000   -1.102000   -0.772000   -0.446000   \n",
            "50%     -0.334000    0.241000    0.251000    0.760000   -0.772000   -0.446000   \n",
            "75%      0.854000    0.839000    0.954000    0.760000    1.294000    0.448000   \n",
            "max      1.717000    1.437000    1.657000    0.760000    1.294000    1.342000   \n",
            "\n",
            "               X7          X8          Y1          Y2  \n",
            "count  613.000000  613.000000  613.000000  613.000000  \n",
            "mean     0.000294   -0.000051    0.000005    0.000010  \n",
            "std      1.000032    0.999974    0.999997    1.000009  \n",
            "min     -1.723000   -1.804000   -1.520000   -1.319000  \n",
            "25%     -0.972000   -1.162000   -0.777000   -0.821000  \n",
            "50%      0.154000    0.121000   -0.484000   -0.520000  \n",
            "75%      1.280000    0.763000    0.956000    0.893000  \n",
            "max      1.280000    1.405000    2.536000    2.525000  \n",
            "        X1     X2     X3     X4     X5     X6     X7     X8     Y1     Y2\n",
            "0    1.717 -1.553  0.251 -1.722  1.294 -1.341 -1.723 -1.804  0.146  0.741\n",
            "1    1.717 -1.553  0.251 -1.722  1.294 -0.446 -1.723 -1.804  0.215  0.397\n",
            "2    1.717 -1.553  0.251 -1.722  1.294  0.448 -1.723 -1.804  0.131  0.371\n",
            "3    1.717 -1.553  0.251 -1.722  1.294  1.342 -1.723 -1.804  0.015  0.898\n",
            "4    1.286 -1.254 -0.452 -1.102  1.294 -1.341 -1.723 -1.804 -0.005  0.625\n",
            "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...\n",
            "608 -1.090  1.138  0.954  0.760 -0.772  1.342  1.280  1.405 -0.187 -0.074\n",
            "609 -1.306  1.437  1.657  0.760 -0.772 -1.341  1.280  1.405 -0.337 -0.610\n",
            "610 -1.306  1.437  1.657  0.760 -0.772 -0.446  1.280  1.405 -0.349 -0.583\n",
            "611 -1.306  1.437  1.657  0.760 -0.772  0.448  1.280  1.405 -0.344 -0.642\n",
            "612 -1.306  1.437  1.657  0.760 -0.772  1.342  1.280  1.405 -0.326 -0.711\n",
            "\n",
            "[613 rows x 10 columns]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-879-0035d266bec1>:19: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  data_drop = data_drop[data_zscore[col] == False]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "#from sklearn import linear_model\n",
        "\n",
        "data = pd.read_excel('https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx')\n",
        "data_zscore = data.copy()\n",
        "data = data.dropna()#Drop any line that includes NULL\n",
        "data = data.drop_duplicates()#Drop any duplicate data\n",
        "cols = data.columns\n",
        "#USe z_score to standardize and remove the outliers\n",
        "for col in cols:\n",
        "    data_col = data[col]\n",
        "    z_score = (data_col - data_col.mean()) / data_col.std()#Standardize the data\n",
        "    data_zscore[col] = z_score.abs()>2\n",
        "data_drop = data\n",
        "for col in cols:\n",
        "    data_drop = data_drop[data_zscore[col] == False]\n",
        "df = data_drop.reset_index(drop=True)#df is the dataset we are going to analyse\n",
        "m = 0\n",
        "print(df.describe())\n",
        "\n",
        "for col in df.columns:#Normalization of the data\n",
        "  df[col]=(df[col].subtract(df[col].mean())).div(df[col].std()).round(3)\n",
        "\n",
        "print(df.describe())\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 880,
      "metadata": {
        "id": "k4BkhrxpLl6x"
      },
      "outputs": [],
      "source": [
        "class TrainingSetGenerator:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def generate(self, data, percent):#percent indicates what percent of data is for training the model\n",
        "        data = data.sample(frac=1.0,random_state = random.randint(1,1000)) #randomize the data\n",
        "        data = data.reset_index(drop=True)  #reset the index\n",
        "        #print(data)\n",
        "\n",
        "        #divide the data into train sets and test sets\n",
        "        select_columns_x = data.columns[0:8]\n",
        "        select_columns_y = data.columns[8:10]\n",
        "        data_x = data[select_columns_x]\n",
        "        data_y = data[select_columns_y]\n",
        "        count = int(data.shape[0]*percent)\n",
        "        self.data_train_x = data_x[0:count+1].values\n",
        "        self.data_train_y = data_y[0:count+1].values\n",
        "        self.data_test_x = data_x[count+1:data.shape[0]].values\n",
        "        self.data_test_y = data_y[count+1:data.shape[0]].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 881,
      "metadata": {
        "id": "7btKQjBBLtGi"
      },
      "outputs": [],
      "source": [
        "class LinearRegression:\n",
        "    def __init__(self, bias=True):\n",
        "        self.bias = bias\n",
        "        self.mse1 = 0\n",
        "        self.mse2 = 0\n",
        "        pass\n",
        "    def fit(self, x, y):\n",
        "        N = x.shape[0]\n",
        "        if(self.bias):\n",
        "            x = np.column_stack([np.ones(N),x])#add one in the front to create the X matrix\n",
        "        self.w = np.linalg.inv(x.T @ x)@x.T@y#Calculating the least square\n",
        "\n",
        "    def predict(self,x):\n",
        "        N = x.shape[0]\n",
        "        if(self.bias):\n",
        "            x = np.column_stack([np.ones(N),x])\n",
        "        yh = x@self.w\n",
        "        return yh\n",
        "\n",
        "    def predict_first(self, X):\n",
        "        N = X.shape[0]\n",
        "        D = X.shape[1]\n",
        "        data = np.c_[np.ones(N), X]\n",
        "        return data @ self.w[:,0].reshape(data.shape[1],1)\n",
        "\n",
        "    def predict_second(self, X):\n",
        "        N = X.shape[0]\n",
        "        D = X.shape[1]\n",
        "        data = np.c_[np.ones(N), X]\n",
        "        return data @ self.w[:,1].reshape(data.shape[1],1)\n",
        "        \n",
        "\n",
        "    def meanSquare(self, X, Y):\n",
        "        Y1 = Y[:,0]\n",
        "        Y2 = Y[:,1]\n",
        "        Y1 = Y1.reshape(X.shape[0],1)\n",
        "        Y2 = Y2.reshape(X.shape[0],1)\n",
        "        error1 = Y1-self.predict_first(X)\n",
        "        error2 = Y2-self.predict_second(X)\n",
        "        print(\"The error for Y1 is: \" + str((error1*error1).mean()))\n",
        "        print(\"The error for Y2 is: \" + str((error2*error2).mean()))\n",
        "        self.mse1 += (error1*error1).mean()\n",
        "        self.mse2 += (error2*error2).mean()\n",
        "\n",
        "    def reset(self):\n",
        "        self.mse1 = 0\n",
        "        self.mse2 = 0\n",
        "\n",
        "\n",
        "generator = TrainingSetGenerator()\n",
        "generator.generate(df,0.5)\n",
        "model = LinearRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 882,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2kQzCorLtNj",
        "outputId": "4dc00fb8-f379-453c-e04b-70502c959c3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The weight and intercept that we have: (The first line is the intercept)\n",
            "The Training is finished. MSE=0.08378697821485745 trained 100000 times\n",
            "The Training is finished. MSE=0.10430964511803553 trained 99892 times\n",
            "[[-0.00937362 -0.00587656]\n",
            " [-1.01601044 -1.13655294]\n",
            " [-0.84569454 -1.00808021]\n",
            " [ 0.12115163  0.10308778]\n",
            " [-0.85804329 -0.959455  ]\n",
            " [ 0.21070412  0.09237515]\n",
            " [-0.00269948  0.01381247]\n",
            " [ 0.23865714  0.16907907]\n",
            " [ 0.03280542  0.01133678]]\n",
            "\n",
            "\n",
            "The MSE for the train set: \n",
            "The error for Y1 is: 0.08378697821485745\n",
            "The error for Y2 is: 0.10430964511803553\n",
            "\n",
            "\n",
            "The MSE for the test set: \n",
            "The error for Y1 is: 0.12564049768127877\n",
            "The error for Y2 is: 0.13357783176937665\n",
            "The weight and intercept that we have: (The first line is the intercept)\n",
            "[[-0.00928493 -0.00583853]\n",
            " [-1.73895181 -1.89097109]\n",
            " [-2.13684654 -2.33889435]\n",
            " [ 0.45319251  0.44318528]\n",
            " [-0.26492249 -0.35532066]\n",
            " [ 0.21138859  0.09306888]\n",
            " [-0.00269948  0.01381247]\n",
            " [ 0.23865714  0.16907907]\n",
            " [ 0.03280542  0.01133678]]\n",
            "\n",
            "\n",
            "The MSE for the train set: \n",
            "The error for Y1 is: 0.09254657721165291\n",
            "The error for Y2 is: 0.11403369833904442\n",
            "\n",
            "\n",
            "The MSE for the test set: \n",
            "The error for Y1 is: 0.12470597165941062\n",
            "The error for Y2 is: 0.127847257635386\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'for i in  range(100):\\n    generator1.generate(df,0.8)\\n    model2.fit(generator1.data_train_x, generator1.data_train_y)\\n    model2.meanSquare(generator1.data_test_x, generator1.data_test_y)\\nprint(model2.mse1/100)\\nprint(model2.mse2/100)'"
            ]
          },
          "execution_count": 882,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LinearRegression_Optimizer:\n",
        "    def __init__(self, max_iters, rate=0.01, e=1e-8, batch_size=0, alg=0, m=0):\n",
        "        self.grad = None    #Gradient\n",
        "        self.w = None   #weight\n",
        "        self.max_iters = max_iters  #max iterating times\n",
        "        self.r = rate   #learning rate\n",
        "        self.rate = self.r  #Learning rate\n",
        "        self.e = e  #epsilon\n",
        "        self.t = 0  #times counter\n",
        "        self.list = []  #randomize for SGD\n",
        "        self.batch_size = batch_size    #batch size\n",
        "        self.sigma = None   #sigma for adagrad\n",
        "        self.mse = np.inf   #mean square error\n",
        "        self.ep = 1e-10 #'bias'\n",
        "        self.momentum = m   #Momentum\n",
        "        self.lastV = 0  #last rate*gradient\n",
        "        self.firstWeight = None\n",
        "        self.secondWeight = None\n",
        "        if alg == 0:    #choose algorithm: simple(change batch_size to use SGD), adaptive, adaptivegradient\n",
        "            self.fitw = self.simple\n",
        "        elif alg == 1:\n",
        "            self.fitw = self.adaptive\n",
        "        elif alg == 2:\n",
        "            self.fitw = self.adagrad\n",
        "        else:\n",
        "            self.fitw = self.simple\n",
        "\n",
        "    def fit_twice(self, X, Y):  #fit the data with Y1 and Y2\n",
        "        Y1 = Y[:,0]\n",
        "        Y2 = Y[:,1]\n",
        "        self.fit(X, Y1)\n",
        "        self.firstWeight = self.w\n",
        "        self.fit(X, Y2)\n",
        "        self.secondWeight = self.w\n",
        "        w = np.append(self.firstWeight, self.secondWeight, axis=1)\n",
        "        print(w)\n",
        "        return w\n",
        "\n",
        "    def fit(self, X, Y):    #fit the data\n",
        "        #data reset\n",
        "        self.t = 0\n",
        "        N = X.shape[0]\n",
        "        D = X.shape[1]+1\n",
        "        self.list = [i for i in range(len(X))]\n",
        "        Y = Y.reshape(N, 1)\n",
        "        data = np.c_[np.ones(len(X)), X] \n",
        "        self.rate = np.c_[np.ones(D)] * self.r\n",
        "        self.mse = np.inf\n",
        "        self.w = np.c_[np.array([0,0,0,0,0,0,0,0,0])]\n",
        "        self.sigma = np.c_[np.zeros(D)]\n",
        "        self.grad = 1\n",
        "        #if meat the max iterating times or norm > epsilon, stop\n",
        "        while self.t < self.max_iters and np.linalg.norm(self.grad) > self.e:\n",
        "            #choose the batch and sample the data(SGD with 1)\n",
        "            batch_X, batch_Y = self.getData(data, Y)\n",
        "            self.grad = self.gradient(batch_X, batch_Y)\n",
        "            #fit the data\n",
        "            self.fitw(self.grad)\n",
        "            self.t += 1\n",
        "            self.mse = self.meanSquareError(X, Y)\n",
        "            '''if(self.mse<0.2):\n",
        "                break'''\n",
        "        print(\"The Training is finished. MSE=\" + str(self.mse) + \" trained \" + str(self.t) + \" times\")\n",
        "\n",
        "    def update(self, v):    #update weight\n",
        "        self.w = self.w - v\n",
        "\n",
        "    def simple(self, g):    #simple method\n",
        "        v = self.rate * g - self.momentum * self.lastV\n",
        "        self.update(v)\n",
        "        self.lastV = v\n",
        "\n",
        "    def adaptive(self, g):  #adaptive method\n",
        "        v = self.rate / pow(self.t + 1, 0.5) * g - self.momentum * self.lastV\n",
        "        self.update(v)\n",
        "        self.lastV = v\n",
        "\n",
        "    def adagrad(self, g):   #adaptive gradient\n",
        "        self.sigma = self.sigma + np.square(g)\n",
        "        v = self.rate / (pow(self.sigma, 0.5) + self.ep) * g - self.momentum * self.lastV\n",
        "        self.update(v)\n",
        "        self.lastV = v\n",
        "\n",
        "    def getData(self, X, Y):    #random and sample data\n",
        "        if self.batch_size > 0:\n",
        "            random.shuffle(self.list)\n",
        "            batch_X = np.array([X[i] for i in range(self.batch_size)])\n",
        "            batch_Y = np.array([Y[i] for i in range(self.batch_size)])\n",
        "            return batch_X, batch_Y\n",
        "        else:\n",
        "            return X, Y\n",
        "\n",
        "    def gradient(self, X, Y):   #gradient formula\n",
        "        return (1 / len(X)) * (X.T @ (X @ self.w - Y))\n",
        "\n",
        "    #meanSquareError\n",
        "    def meanSquareError(self, X, Y):\n",
        "        N = X.shape[0]\n",
        "        D = X.shape[1]\n",
        "        Y = Y.reshape(N, 1)\n",
        "        error = Y - self.predict(X)\n",
        "        return (error * error).mean()\n",
        "    #predict the data according to current weight\n",
        "    def predict(self, X):\n",
        "        N = X.shape[0]\n",
        "        D = X.shape[1]\n",
        "        data = np.c_[np.ones(N), X]\n",
        "        return data @ self.w\n",
        "\n",
        "    def predict_first(self, X):\n",
        "        N = X.shape[0]\n",
        "        D = X.shape[1]\n",
        "        data = np.c_[np.ones(N), X]\n",
        "        return data @ self.firstWeight\n",
        "\n",
        "    def predict_second(self, X):\n",
        "        N = X.shape[0]\n",
        "        D = X.shape[1]\n",
        "        data = np.c_[np.ones(N), X]\n",
        "        return data @ self.secondWeight\n",
        "\n",
        "\n",
        "    def meanSquare(self, X, Y):\n",
        "        Y1 = Y[:,0]\n",
        "        Y2 = Y[:,1]\n",
        "        Y1 = Y1.reshape(X.shape[0],1)\n",
        "        Y2 = Y2.reshape(X.shape[0],1)\n",
        "        error1 = Y1-self.predict_first(X)\n",
        "        error2 = Y2-self.predict_second(X)\n",
        "        print(\"The error for Y1 is: \" + str((error1*error1).mean()))\n",
        "        print(\"The error for Y2 is: \" + str((error2*error2).mean()))\n",
        "\n",
        "#def __init__(self, max_iters, rate=0.01, e=1e-8, batch_size=0, alg=0, m=0)\n",
        "\n",
        "\n",
        "\n",
        "#Task3\n",
        "generator1 = TrainingSetGenerator()\n",
        "generator1.generate(df,0.8)\n",
        "model1 = LinearRegression_Optimizer(100000,0.6,1e-10, 0, 2, 0.5)\n",
        "print(\"The weight and intercept that we have: (The first line is the intercept)\")\n",
        "model1.fit_twice(generator1.data_train_x, generator1.data_train_y)\n",
        "print(\"\\n\")\n",
        "print(\"The MSE for the train set: \")\n",
        "model1.meanSquare(generator1.data_train_x, generator1.data_train_y)\n",
        "print(\"\\n\")\n",
        "print(\"The MSE for the test set: \")\n",
        "model1.meanSquare(generator1.data_test_x, generator1.data_test_y)\n",
        "\n",
        "model2 = LinearRegression()\n",
        "model2.fit(generator1.data_train_x, generator1.data_train_y)\n",
        "print(\"The weight and intercept that we have: (The first line is the intercept)\")\n",
        "print(model2.w)\n",
        "print(\"\\n\")\n",
        "print(\"The MSE for the train set: \")\n",
        "model2.meanSquare(generator1.data_train_x, generator1.data_train_y)\n",
        "print(\"\\n\")\n",
        "print(\"The MSE for the test set: \")\n",
        "model2.meanSquare(generator1.data_test_x, generator1.data_test_y)\n",
        "\n",
        "'''for i in  range(100):\n",
        "    generator1.generate(df,0.8)\n",
        "    model2.fit(generator1.data_train_x, generator1.data_train_y)\n",
        "    model2.meanSquare(generator1.data_test_x, generator1.data_test_y)\n",
        "print(model2.mse1/100)\n",
        "print(model2.mse2/100)'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICrdbZNeSRj7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PZMwZAcOy3j"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "82a6c2361418f74de7ee2095be3edd987aa80c4d72c438b9ad6b623da9ebbb5c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
